---
title: "Poker Rule Induction"
author: "Jagdeep S. Sihota"
---

The intent of this project is to learn poker rules by applying following 5 five learning algorithms:

- Decision trees with some form of pruning
- Neural networks
- Boosting
- Support Vector Machines
- k-nearest neighbors

This dataset is interesting becasue we need to predict the best hand you can play based on the cards you've been dealt without hand coding rules. The order of cards is also important, which means there are 480 possible Royal Flush hands instead of just four. Identify those, and the other 311,875,200 possible hands correctly. Pretend you  never played the game before, are given a history of thousands of games, and are asked to come up with the rules. It is potentially difficult to discover rules that can correctly classify poker hands. Algorithm will need to find rules that are general enough to be broadly useful, without being so broad that they end up being occasionally wrong.




# Data
Each record in this dataset consists of five playing cards and an attribute representing the poker hand. You are provided with 25,010 poker hands in train.csv and 1,000,000 in test.csv. Each hand consists of five cards with a given suit and rank, drawn from a standard deck of 52. Suits and ranks are represented as ordinal categories:

Each row in the training set has the accompanying class label for the poker hand it comprises. The hands are omitted from the test set and must be predicted by participants. Hands are classified into the following ordinal categories:

Attribute Information:

1) S1 "Suit of card #1" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

2) C1 "Rank of card #1" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

3) S2 "Suit of card #2" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

4) C2 "Rank of card #2" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

5) S3 "Suit of card #3" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

6) C3 "Rank of card #3" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

7) S4 "Suit of card #4" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

8) C4 "Rank of card #4" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

9) S5 "Suit of card #5" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

10) C5 "Rank of card 5" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

11) CLASS "Poker Hand" 
Ordinal (0-9) 

Class Information:

- 0: Nothing in hand; not a recognized poker hand 
- 1: One pair; one pair of equal ranks within five cards 
- 2: Two pairs; two pairs of equal ranks within five cards 
- 3: Three of a kind; three equal ranks within five cards 
- 4: Straight; five cards, sequentially ranked with no gaps 
- 5: Flush; five cards with the same suit 
- 6: Full house; pair + different rank three of a kind 
- 7: Four of a kind; four equal ranks within five cards 
- 8: Straight flush; straight + flush 
- 9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush 



```{r, echo=FALSE}
setwd("~/git/GTMachineLearning_Assignment_1/rawData")
train = read.csv("train.csv")
test = read.csv("test.csv")


test = test[,2:12]
train = train[,2:12]
#Separate labels from training set
trainlabels = as.factor(train$CLASS)
train = train[,1:10]
testlabels = as.factor(test$CLASS)
test = test[,1:10]

#Split training set into partial training set and validation set
part_train = train[1:18000,]
valid = train[-1:-18000,]
labels_part = trainlabels[1:18000]
valid_labels = trainlabels[-1:-18000]

```

# Decision Trees
## decision tree algorithm
### bias-variance tradeoff
decision trees of depth D: increasing D decision trees of depth D: increasing D
typically increases variance and reduces typically increases variance and reduces
bias
Models that can fit the data very well have low Models that can fit the data very well have low
bias but high variance: bias but high variance:
“flexible flexible
” models such as models such as
nearest neighbor regression, regression trees nearest neighbor regression, regression trees

###  preference and restriction bias in each of the these algorithms
restriction bias  all possible trees
preference = inductive bias 
it pref good split at near the top. it pref correct ones to incoret ones. shorter tree to longer trees 

Run decion tree 

```{r, echo=FALSE}
library(rpart)
set.seed(12)
mycontrol = rpart.control(minsplit=3, minbucket=1, cp=0.001)
tree = rpart(labels_part~.,method="class", data=part_train,control = mycontrol)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(tree)
plotcp(tree)

```
## pruning
### describe whatever it is that you do used for pruning
Prune back the tree to avoid overfitting the data. Typically, you will want to select a tree size that minimizes the cross-validated error.     fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"]
to automatically select the complexity parameter associated with the smallest cross-validated error
```{r, echo=FALSE}
pfit<- prune(tree, cp=0.0022)
plotcp(pfit) 
fancyRpartPlot(pfit)
```

## Generalization Error
### error function when it is a classification problem
## cross-validation and when do you use it? What are the other forms of validation techniques
```{r, echo=FALSE}
library(caret)
tree_pred = predict(pfit, newdata=valid, type="class")
confusionMatrix(tree_pred,valid_labels)
```
## how can you go about making your model more complex? Draw model complexity graphs for each of them. Analyze these graphs.
## How can we use learning curves to measure the performance of a machine learning algorithm? Draw learning curve graphs for each of them. Analyze these graphs.
```{r, echo=FALSE}
pred = predict(pfit, newdata=test, type="class")
confusionMatrix(pred,testlabels)
```

# Neural Networks
## algorithm

### bias-variance tradeoff
###  preference and restriction bias in each of the these algorithms
One of the most serious problems that arises in connectionist learning by neural networks is overfitting of the provided training examples. This means that the learned function fits very closely the training data however it does not generalise well, that is it can not model sufficiently well unseen data from the same task.A neural network that fits closely the provided training examples has a low bias but a high variance. If we reduce the network variance this will lead to a decrease in the level of fitting the data.When using neural networks, small neural networks are more prone to under-fitting and big neural networks are prone to over-fitting. Cross-validation of network size is a way to choose alternatives.

## Generalization Error
### error function when it is a classification problem
```{r, echo=FALSE}
library(nnet)
fit <- nnet(labels_part ~ ., data=part_train,size=6,decay=5e-4)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(fit)
```

## cross-validation and when do you use it? What are the other forms of validation techniques
```{r, echo=FALSE}
library(caret)
pred = predict(fit, valid, type = "class")
table(valid_labels, pred)
```

## how can you go about making your model more complex? Draw model complexity graphs for each of them. Analyze these graphs.
```{r, echo=FALSE}
pred = predict(fit, test, type = "class")
table(testlabels, pred)

```

## How can we use learning curves to measure the performance of a machine learning algorithm? Draw learning curve graphs for each of them. Analyze these graphs.

# Boosting
## algorithm
### bias-variance tradeoff
###  preference and restriction bias in each of the these algorithms
```{r, echo=FALSE}
library(gbm)
set.seed(12)
tunecontrol = trainControl(method = "none")
tgrid = expand.grid(n.trees = c(100), interaction.depth=c(15) ,shrinkage=c(0.107) )
gbm_mod = train(labels_part~., data=part_train, method="gbm", trControl=tunecontrol, tuneGrid=tgrid)
```
## Generalization Error
### error function when it is a classification problem
## cross-validation and when do you use it? What are the other forms of validation techniques
```{r, echo=FALSE}
pred_gbm = predict(gbm_mod, newdata=valid)
confusionMatrix(pred_gbm ,valid_labels)
```
## how can you go about making your model more complex? Draw model complexity graphs for each of them. Analyze these graphs.
```{r, echo=FALSE}
pred = predict(gbm_mod, newdata=test)
confusionMatrix(pred,testlabels)

```
## How can we use learning curves to measure the performance of a machine learning algorithm? Draw learning curve graphs for each of them. Analyze these graphs.

# Support Vector Machines
## algorithm
# ```{r, echo=FALSE}
# library(kernlab)
# svm <- ksvm(train[,11:11] ~ ., data = train[,1:10],type = "C-svc")
# 
# ```
### bias-variance tradeoff
###  preference and restriction bias in each of the these algorithms
#swap out kernel functions. I'd like to see at least two.
## Generalization Error
### error function when it is a classification problem
## cross-validation and when do you use it? What are the other forms of validation techniques
## how can you go about making your model more complex? Draw model complexity graphs for each of them. Analyze these graphs.
## How can we use learning curves to measure the performance of a machine learning algorithm? Draw learning curve graphs for each of them. Analyze these graphs.


# k-Nearest Neighbors
## algorithm
## You should implement kNN. Use different values of k.
### bias-variance tradeoff
###  preference and restriction bias in each of the these algorithms
## Generalization Error
### error function when it is a classification problem
## cross-validation and when do you use it? What are the other forms of validation techniques
## how can you go about making your model more complex? Draw model complexity graphs for each of them. Analyze these graphs.
## How can we use learning curves to measure the performance of a machine learning algorithm? Draw learning curve graphs for each of them. Analyze these graphs.


# Conclude all the experiments you did in 2 and 3. What is the “best” hypothesis that describes your dataset? Why?
