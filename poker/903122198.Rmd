```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```
---
title: "Poker Rule Induction"
author: "Jagdeep S. Sihota"
---

The intent of this project is to learn poker rules by applying following 5 five learning algorithms:

- Decision trees with some form of pruning
- Neural networks
- Boosting
- Support Vector Machines
- k-nearest neighbors

We need to predict the best hand you can play based on the cards you've been dealt without hand coding rules. The order of cards is also important, which means there are 480 possible Royal Flush hands instead of just four. Identify those, and the other 311,875,200 possible hands correctly. Pretend you  never played the game before, are given a history of thousands of games, and are asked to come up with the rules. It is potentially difficult to discover rules that can correctly classify poker hands. Algorithm will need to find rules that are general enough to be broadly useful, without being so broad that they end up being occasionally wrong.

# Data
Each record in this dataset consists of five playing cards and an attribute representing the poker hand. We are provided with 25,010 poker hands in train.csv and 1,000,000 in test.csv. Each hand consists of five cards with a given suit and rank, drawn from a standard deck of 52. Suits and ranks are represented as ordinal categories:

Each row in the training set has the accompanying class label for the poker hand it comprises. The hands are omitted from the test set and must be predicted by participants. Hands are classified into the following ordinal categories:

Attribute Information:

1) S1 "Suit of card #1" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

2) C1 "Rank of card #1" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

3) S2 "Suit of card #2" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

4) C2 "Rank of card #2" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

5) S3 "Suit of card #3" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

6) C3 "Rank of card #3" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

7) S4 "Suit of card #4" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

8) C4 "Rank of card #4" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

9) S5 "Suit of card #5" 
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs} 

10) C5 "Rank of card 5" 
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King) 

11) CLASS "Poker Hand" 
Ordinal (0-9) 

Class Information:

- 0: Nothing in hand; not a recognized poker hand 
- 1: One pair; one pair of equal ranks within five cards 
- 2: Two pairs; two pairs of equal ranks within five cards 
- 3: Three of a kind; three equal ranks within five cards 
- 4: Straight; five cards, sequentially ranked with no gaps 
- 5: Flush; five cards with the same suit 
- 6: Full house; pair + different rank three of a kind 
- 7: Four of a kind; four equal ranks within five cards 
- 8: Straight flush; straight + flush 
- 9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush 



```{r, echo=FALSE, warning=FALSE, message=FALSE}
options(warn=-1)
setwd("~/git/GTMachineLearning_Assignment_1/rawData")
train = read.csv("train.csv")
test = read.csv("test.csv")


test = test[,2:12]
train = train[,2:12]
#Separate labels from training set
trainlabels = as.factor(train$CLASS)
train = train[,1:10]
testlabels = as.factor(test$CLASS)
test = test[,1:10]

#Split training set into partial training set and validation set
part_train = train[1:18000,]
valid = train[-1:-18000,]
labels_part = trainlabels[1:18000]
valid_labels = trainlabels[-1:-18000]

```

# Decision Trees
Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item's target value. Tree models where the target variable can take a finite set of values are called classification trees. In these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision tree restriction bias is discrete because we only consider functions that can be represented by decision tree. For preferences bias Decision prefer good splits near the top than a tree that has bad splits, model the data better to ones that model the data worse and prefer shorter trees to longer trees.

Let's create decision tree using  recursive partitioning.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(rpart)
set.seed(12)
mycontrol = rpart.control(minsplit=3, minbucket=1, cp=0.001)
tree = rpart(labels_part~.,method="class", data=part_train,control = mycontrol)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(tree)
plotcp(tree)

```

As we can see tree is large and there is not much difference in cross validation. We will prune using complexity parameter with minimum error.
Prune back the tree to avoid overfitting the data.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
pfit<- prune(tree, cp=0.0022)
plotcp(pfit) 
fancyRpartPlot(pfit)
```

Using pruning we reduce the size of tree and avoided overfittin. Know let's predict using the above decssion tree. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
pred = predict(pfit, newdata=test, type="class")
library(caret)
cm = confusionMatrix(pred,testlabels)
cm
```

With decision tree we got  54% accuracy.

# Neural Networks
Neural networks are relatively crude electronic networks of "neurons" based on the neural structure of the brain.  They process records one at a time, and "learn" by comparing their classification of the record (which, at the outset, is largely arbitrary) with the known actual classification of the record.  The errors from the initial classification of the first record is fed back into the network, and used to modify the networks algorithm the second time around, and so on for many iterations. 
One of the most serious problems that arises in connectionist learning by neural networks is overfitting of the provided training examples. This means that the learned function fits very closely the training data however it does not generalise well, that is it can not model sufficiently well unseen data from the same task.A neural network that fits closely the provided training examples has a low bias but a high variance. If we reduce the network variance this will lead to a decrease in the level of fitting the data.When using neural networks, small neural networks are more prone to under-fitting and big neural networks are prone to over-fitting. Cross-validation of network size is a way to choose alternatives.

Let's create neural network using feed-forward neaural network with hidden layer 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(nnet)
fit <- nnet(labels_part ~ ., data=part_train,size=6,decay=5e-4)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(fit)
```

Now let's predict using the above trained network.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
pred = predict(fit, test, type = "class")
table(testlabels, pred)
```
With decision tree we got  57% accuracy.

# Boosting
Boosting is a machine learning ensemble meta-algorithm for reducing bias primarily and also variance[1] in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones.  A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification. While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are typically weighted in some way that is usually related to the weak learners' accuracy. After a weak learner is added, the data is reweighted: examples that are misclassified gain weight and examples that are classified correctly lose weight (some boosting algorithms actually decrease the weight of repeatedly misclassified examples, e.g., boost by majority and BrownBoost). Thus, future weak learners focus more on the examples that previous weak learners misclassified.

Bias
– models with too few parameters can perform poorly
– models with too many parameters can perform poorly
– a model which is too simple, or too inflexible, will have a large bias
– a model which has too much flexibility will have high variance

Let's use AdaBoost.M1 uing classification trees and silgle classifires 
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(gbm)
set.seed(12)
tunecontrol = trainControl(method = "none")
tgrid = expand.grid(n.trees = c(100), interaction.depth=c(15) ,shrinkage=c(0.107) )
gbm_mod = train(labels_part~., data=part_train, method="gbm", trControl=tunecontrol, tuneGrid=tgrid)
gbm_mod
```

Now let's predict using the above boosting modal.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
pred = predict(gbm_mod, newdata=test)
confusionMatrix(pred,testlabels)

```

With boosting we got  65% accuracy.

# Support Vector Machines
In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.

The bias-variance tradeoff provides insight into their success. Typical classes in text classification are complex and seem unlikely to be modeled well linearly. However, this intuition is misleading for the high-dimensional spaces that we typically encounter in text applications. With increased dimensionality, the likelihood of linear separability increases rapidly (Exercise 14.8 ). Thus, linear models in high-dimensional spaces are quite powerful despite their linearity. Even more powerful nonlinear learning methods can model decision boundaries that are more complex than a hyperplane, but they are also more sensitive to noise in the training data. Nonlinear learning methods sometimes perform better if the training set is large, but by no means in all cases.

We will be using two kernel functions 

1. radial
2. linear

using sampling method: 10-fold cross validation 
- best parameters:
  gamma cost
 0.1    1

## radial kernal
```{r, echo=FALSE, warning=FALSE, message=FALSE}
cat("\014") 
setwd("~/git/GTMachineLearning_Assignment_1/rawData")
#setwd("~/class/GTMachineLearning_Assignment_1/rawData")
train = read.csv("train.csv")
test = read.csv("test.csv")
test = test[,2:12]
train = train[,2:12]
trainCLASS = as.factor(train$CLASS)
library(e1071)
model  <- svm(trainCLASS~., data = train, kernel="radial", gamma=0.1, cost=1)
summary(model)

```

Let's predict using radial kernal function

```{r, echo=FALSE, warning=FALSE, message=FALSE}
prediction <- predict(model, test)
tab <- table(pred = prediction, true = test$CLASS)
tab
```

## radial linear
```{r, echo=FALSE, warning=FALSE, message=FALSE}

library(e1071)
model  <- svm(trainCLASS~., data = train, kernel="linear", gamma=0.1, cost=1)
summary(model)

```

Let's predict using radial linear function

```{r, echo=FALSE, warning=FALSE, message=FALSE}
prediction <- predict(model, test)
tab <- table(pred = prediction, true = test$CLASS)
tab
```

# k-Nearest Neighbors
In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression. In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.Both for classification and regression, it can be useful to weight the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.[2] The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.

A bias of the k-NN algorithm is that it is sensitive to the local structure of the data.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(class)

knn_predict <-knn(train, test, trainCLASS, k = 10, l = 0, prob = FALSE, use.all = TRUE)
table(knn_predict, test$CLASS)
#confusionMatrix(knn_predict,test$CLASS)
```

# Conclude all the experiments you did in 2 and 3. What is the “best” hypothesis that describes your dataset? Why?

# References 
http://en.wikipedia.org/wiki/Decision_tree_learning
http://www.solver.com/xlminer/help/neural-networks-classification-intro
http://en.wikipedia.org/wiki/Boosting_%28machine_learning%29